{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcea1b-4ebf-4b7c-b2cc-110553b71657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from scipy.stats import skew\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score, KFold, StratifiedKFold,\n",
    "    train_test_split, RandomizedSearchCV, GridSearchCV)\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, confusion_matrix, roc_auc_score,\n",
    "    RocCurveDisplay, accuracy_score, f1_score, classification_report,\n",
    "    balanced_accuracy_score, fbeta_score, precision_recall_curve, roc_curve, PrecisionRecallDisplay)\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder,\n",
    "    OrdinalEncoder, PowerTransformer)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612392e-4faf-4564-8584-5bfac98de619",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98e421-854d-4465-b9ce-7cb7b40005e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base data\n",
    "X_train = pd.read_pickle('X_train.pkl')\n",
    "X_test = pd.read_pickle('X_test.pkl')\n",
    "y_train = pd.read_pickle('y_train.pkl')\n",
    "y_test = pd.read_pickle('y_test.pkl')\n",
    "\n",
    "# load age & sex matched data\n",
    "X_train_matched = pd.read_pickle('X_train_matched.pkl')\n",
    "X_test_matched = pd.read_pickle('X_test_matched.pkl')\n",
    "y_train_matched = pd.read_pickle('y_train_matched.pkl')\n",
    "y_test_matched = pd.read_pickle('y_test_matched.pkl')\n",
    "\n",
    "\n",
    "n_iterations = 100     # for hyperparmeter tuning\n",
    "cv_folds = 5           # number of cross validation folds for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8420f0b-ab62-4b5d-ae6c-0244b305d185",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Scoring metric = ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40950074-c9ba-4fd6-9b79-497f7e1b5850",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# base data hyperparameter tuning\n",
    "def objective(trial):\n",
    "    # hyperparameters relevant to LGBM\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_bin': trial.suggest_int('max_bin', 128, 1024),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 128),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 1000, log=True),\n",
    "        'min_sum_hessian_in_leaf': trial.suggest_float('min_sum_hessian_in_leaf', 1e-4, 10, log=True),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 5000),\n",
    "        'n_jobs': -1,\n",
    "        'bagging_freq': 1,\n",
    "        'force_row_wise': True,\n",
    "        'bagging_seed': 2024,\n",
    "        'verbosity': -100,\n",
    "        'extra_trees': False}\n",
    "\n",
    "    # create model with above parameters\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    cv = KFold(n_splits = cv_folds, shuffle=True, random_state=1)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=1)\n",
    "    return scores.mean()\n",
    "\n",
    "# create and run the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=n_iterations, show_progress_bar=True)\n",
    "\n",
    "# get the best hyperparameters\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"\\nBest cross-validation score: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313cd534-8b1f-45e9-ba5f-4a15286153cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# base data model evaluation\n",
    "\n",
    "# set the optimal hyperparameters found above\n",
    "base_params = {\n",
    "    'n_estimators': 368,\n",
    "    'max_bin': 612,\n",
    "    'learning_rate': 0.04492929029938169,\n",
    "    'max_depth': 4,\n",
    "    'num_leaves': 69,\n",
    "    'bagging_fraction': 0.8930333984571698,\n",
    "    'feature_fraction': 0.6723092127660762,\n",
    "    'lambda_l2': 101.66071203673786,\n",
    "    'min_sum_hessian_in_leaf': 0.0035990279823698043,\n",
    "    'min_data_in_leaf': 24,\n",
    "\n",
    "    # normal params:\n",
    "    'n_jobs': -1,\n",
    "    'bagging_freq': 1,\n",
    "    'force_row_wise': True,\n",
    "    'bagging_seed': 2024,\n",
    "    'verbosity': -100,\n",
    "    'extra_trees': False,\n",
    "}\n",
    "\n",
    "# train the final model\n",
    "base_model = lgb.LGBMClassifier(**base_params)\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions with test set\n",
    "y_pred = base_model.predict(X_test)\n",
    "y_pred_proba = base_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# eval metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"AUROC: {auroc:.4f}\")\n",
    "print(\"\\nConfusion Matrix \\nTN FP \\nFN TP:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# feature importance\n",
    "feature_importance = base_model.feature_importances_\n",
    "feature_names = X_train.columns if hasattr(X_train, 'columns') else [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "plt.figure(figsize=(3,3))\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='0.5')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# precision recall curve\n",
    "plt.figure(figsize=(3,3))\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_pred_proba)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf4dcb-5928-4099-96a2-56d5e3b08bbd",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb4746-3826-41ba-9eca-33c6581738d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# undersampling controls\n",
    "undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state=1)\n",
    "X_train_smote, y_train_smote = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"before undersampling:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nafter undersampling:\")\n",
    "print(y_train_smote.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8ebaf-3913-489e-8b48-446d7e66e0a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# undersampled hyperparameter tuning\n",
    "\n",
    "def objective(trial):\n",
    "    # hyperparameters relevant to LGBM\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_bin': trial.suggest_int('max_bin', 128, 1024),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 128),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 1000, log=True),\n",
    "        'min_sum_hessian_in_leaf': trial.suggest_float('min_sum_hessian_in_leaf', 1e-4, 10, log=True),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 5000),\n",
    "        'n_jobs': -1,\n",
    "        'bagging_freq': 1,\n",
    "        'force_row_wise': True,\n",
    "        'bagging_seed': 2024,\n",
    "        'verbosity': -100,\n",
    "        'extra_trees': False}\n",
    "\n",
    "    # create model with above parameters\n",
    "    model_smote = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    cv = KFold(n_splits = cv_folds, shuffle=True, random_state=1)\n",
    "    scores = cross_val_score(model_smote, X_train_smote, y_train_smote, cv=cv, scoring='roc_auc', n_jobs=1)\n",
    "    return scores.mean()\n",
    "\n",
    "# create and run the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=n_iterations, show_progress_bar=True)\n",
    "\n",
    "# get the best hyperparameters\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"\\nBest cross-validation score: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2724a39-7749-4d38-938f-4cf8d505e982",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# undersampled model evaluation\n",
    "\n",
    "# set the optimal hyperparameters found above\n",
    "smote_params = {\n",
    "    'n_estimators': 381,\n",
    "    'max_bin': 369,\n",
    "    'learning_rate': 0.030027550861638556,\n",
    "    'max_depth': 3,\n",
    "    'num_leaves': 87,\n",
    "    'bagging_fraction': 0.9336151881344432,\n",
    "    'feature_fraction': 0.6670365046716105,\n",
    "    'lambda_l2': 0.6386045468782642,\n",
    "    'min_sum_hessian_in_leaf': 0.01048928153287772,\n",
    "    'min_data_in_leaf': 687,\n",
    "    \n",
    "    # normal params:\n",
    "    'n_jobs': -1,\n",
    "    'bagging_freq': 1,\n",
    "    'force_row_wise': True,\n",
    "    'bagging_seed': 2024,\n",
    "    'verbosity': -100,\n",
    "    'extra_trees': False}\n",
    "\n",
    "# train the final model\n",
    "smote_model = lgb.LGBMClassifier(**smote_params)\n",
    "smote_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# make predictions with test set\n",
    "y_pred = smote_model.predict(X_test)\n",
    "y_pred_proba = smote_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# eval metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"AUROC: {auroc:.4f}\")\n",
    "print(\"\\nConfusion Matrix \\nTN FP \\nFN TP:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# feature importance\n",
    "feature_importance = smote_model.feature_importances_\n",
    "feature_names = X_train_smote.columns if hasattr(X_train, 'columns') else [f'Feature_{i}' for i in range(X_train_smote.shape[1])]\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "plt.figure(figsize=(3,3))\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='0.5')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# precision recall curve\n",
    "plt.figure(figsize=(3,3))\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_pred_proba)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2880e3d-a3c2-4b47-b22c-97576d3a0ef1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e6ad6d-5510-41fe-a82e-d1f1af8c8946",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# age/sex matched hyperparameter tuning\n",
    "\n",
    "def objective(trial):\n",
    "    # hyperparameters relevant to LGBM\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_bin': trial.suggest_int('max_bin', 128, 1024),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 128),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 1000, log=True),\n",
    "        'min_sum_hessian_in_leaf': trial.suggest_float('min_sum_hessian_in_leaf', 1e-4, 10, log=True),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 5000),\n",
    "        'n_jobs': -1,\n",
    "        'bagging_freq': 1,\n",
    "        'force_row_wise': True,\n",
    "        'bagging_seed': 2024,\n",
    "        'verbosity': -100,\n",
    "        'extra_trees': False}\n",
    "\n",
    "    # create model with above parameters\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    cv = KFold(n_splits = cv_folds, shuffle=True, random_state=1)\n",
    "    scores = cross_val_score(model, X_train_matched, y_train_matched, cv=cv, scoring='roc_auc', n_jobs=1)\n",
    "    return scores.mean()\n",
    "\n",
    "# create and run the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=n_iterations, show_progress_bar=True)\n",
    "\n",
    "# get the best hyperparameters\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"\\nBest cross-validation score: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb39d01-3737-4709-92cb-70dad9679a4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# age/sex matched model evaluation\n",
    "\n",
    "# set the optimal hyperparameters found above\n",
    "matched_params = {\n",
    "    'n_estimators': 297,\n",
    "    'max_bin': 698,\n",
    "    'learning_rate': 0.03664083987716067,\n",
    "    'max_depth': 7,\n",
    "    'num_leaves': 61,\n",
    "    'bagging_fraction': 0.9609593575826182,\n",
    "    'feature_fraction': 0.5204793037683734,\n",
    "    'lambda_l2': 154.4619926699515,\n",
    "    'min_sum_hessian_in_leaf': 0.004812351346173821,\n",
    "    'min_data_in_leaf': 2202,\n",
    "\n",
    "    # normal params:\n",
    "    'n_jobs': -1,\n",
    "    'bagging_freq': 1,\n",
    "    'force_row_wise': True,\n",
    "    'bagging_seed': 2024,\n",
    "    'verbosity': -100,\n",
    "    'extra_trees': False,\n",
    "}\n",
    "\n",
    "# train the final model\n",
    "matched_model = lgb.LGBMClassifier(**matched_params)\n",
    "matched_model.fit(X_train_matched, y_train_matched)\n",
    "\n",
    "# make predictions with test set\n",
    "y_pred = matched_model.predict(X_test_matched)\n",
    "y_pred_proba = matched_model.predict_proba(X_test_matched)[:, 1]\n",
    "\n",
    "# eval metrics\n",
    "precision = precision_score(y_test_matched, y_pred)\n",
    "recall = recall_score(y_test_matched, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test_matched, y_pred)\n",
    "auroc = roc_auc_score(y_test_matched, y_pred_proba)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"AUROC: {auroc:.4f}\")\n",
    "print(\"\\nConfusion Matrix \\nTN FP \\nFN TP:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# feature importance\n",
    "feature_importance = matched_model.feature_importances_\n",
    "feature_names = X_train_matched.columns if hasattr(X_train_matched, 'columns') else [f'Feature_{i}' for i in range(X_train_matched.shape[1])]\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "plt.figure(figsize=(3,3))\n",
    "RocCurveDisplay.from_predictions(y_test_matched, y_pred_proba)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='0.5')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# precision recall curve\n",
    "plt.figure(figsize=(3,3))\n",
    "PrecisionRecallDisplay.from_predictions(y_test_matched, y_pred_proba)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e9d02b-1568-467f-8d45-38fd59cddebd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Scoring metric = Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725c0e7-8250-4e80-8962-c955c22bbb7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# base data hyperparameter tuning\n",
    "\n",
    "def objective(trial):\n",
    "    # hyperparameters relevant to LGBM\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_bin': trial.suggest_int('max_bin', 128, 1024),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 128),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 1000, log=True),\n",
    "        'min_sum_hessian_in_leaf': trial.suggest_float('min_sum_hessian_in_leaf', 1e-4, 10, log=True),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 5000),\n",
    "        'n_jobs': -1,\n",
    "        'bagging_freq': 1,\n",
    "        'force_row_wise': True,\n",
    "        'bagging_seed': 2024,\n",
    "        'verbosity': -100,\n",
    "        'extra_trees': False}\n",
    "\n",
    "    # create model with above parameters\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    cv = KFold(n_splits = cv_folds, shuffle=True, random_state=1)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='recall', n_jobs=1)\n",
    "    return scores.mean()\n",
    "\n",
    "# create and run the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=n_iterations, show_progress_bar=True)\n",
    "\n",
    "# get the best hyperparameters\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"\\nBest cross-validation score: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f5e00-4eb8-4806-9584-132c5563ac23",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# base data model evaluation\n",
    "\n",
    "# set the optimal hyperparameters found above\n",
    "base_params = {\n",
    "    'n_estimators': 299,\n",
    "    'max_bin': 352,\n",
    "    'learning_rate': 0.2997976527843294,\n",
    "    'max_depth': 9,\n",
    "    'num_leaves': 105,\n",
    "    'bagging_fraction': 0.8152704911597977,\n",
    "    'feature_fraction': 0.8378133204831323,\n",
    "    'lambda_l2': 1.4869979936367044,\n",
    "    'min_sum_hessian_in_leaf': 0.4590025239961025,\n",
    "    'min_data_in_leaf': 261,\n",
    "\n",
    "    # normal params:\n",
    "    'n_jobs': -1,\n",
    "    'bagging_freq': 1,\n",
    "    'force_row_wise': True,\n",
    "    'bagging_seed': 2024,\n",
    "    'verbosity': -100,\n",
    "    'extra_trees': False,\n",
    "}\n",
    "\n",
    "# train the final model\n",
    "base_model = lgb.LGBMClassifier(**base_params)\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions with test set\n",
    "y_pred = base_model.predict(X_test)\n",
    "y_pred_proba = base_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# eval metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"AUROC: {auroc:.4f}\")\n",
    "print(\"\\nConfusion Matrix \\nTN FP \\nFN TP:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# feature importance\n",
    "feature_importance = base_model.feature_importances_\n",
    "feature_names = X_train.columns if hasattr(X_train, 'columns') else [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "plt.figure(figsize=(3,3))\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='0.5')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# precision recall curve\n",
    "plt.figure(figsize=(3,3))\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_pred_proba)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2d2f4-1ca1-4a52-adac-01dba395cd4a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36fdc1-8204-47b1-b3ab-6cb32ec9d7d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# undersampled hyperparameter tuning\n",
    "\n",
    "def objective(trial):\n",
    "    # hyperparameters relevant to LGBM\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_bin': trial.suggest_int('max_bin', 128, 1024),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 128),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 1000, log=True),\n",
    "        'min_sum_hessian_in_leaf': trial.suggest_float('min_sum_hessian_in_leaf', 1e-4, 10, log=True),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 5000),\n",
    "        'n_jobs': -1,\n",
    "        'bagging_freq': 1,\n",
    "        'force_row_wise': True,\n",
    "        'bagging_seed': 2024,\n",
    "        'verbosity': -100,\n",
    "        'extra_trees': False}\n",
    "\n",
    "    # create model with above parameters\n",
    "    model_smote = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    cv = KFold(n_splits = cv_folds, shuffle=True, random_state=1)\n",
    "    scores = cross_val_score(model_smote, X_train_smote, y_train_smote, cv=cv, scoring='recall', n_jobs=1)\n",
    "    return scores.mean()\n",
    "\n",
    "# create and run the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=n_iterations, show_progress_bar=True)\n",
    "\n",
    "# get the best hyperparameters\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"\\nBest cross-validation score: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3bc797-f776-4ac7-89ff-159ed8fae98a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# undersampled model evaluation\n",
    "\n",
    "# set the optimal hyperparameters found above\n",
    "smote_params = {\n",
    "    'n_estimators': 316,\n",
    "    'max_bin': 155,\n",
    "    'learning_rate': 0.09157073221081566,\n",
    "    'max_depth': 8,\n",
    "    'num_leaves': 71,\n",
    "    'bagging_fraction': 0.8910733061393177,\n",
    "    'feature_fraction': 0.8656518984470933,\n",
    "    'lambda_l2': 993.4066861978613,\n",
    "    'min_sum_hessian_in_leaf': 0.45634063289049837,\n",
    "    'min_data_in_leaf': 1095,\n",
    "    \n",
    "    # normal params:\n",
    "    'n_jobs': -1,\n",
    "    'bagging_freq': 1,\n",
    "    'force_row_wise': True,\n",
    "    'bagging_seed': 2024,\n",
    "    'verbosity': -100,\n",
    "    'extra_trees': False}\n",
    "\n",
    "# train the final model\n",
    "smote_model = lgb.LGBMClassifier(**smote_params)\n",
    "smote_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# make predictions with test set\n",
    "y_pred = smote_model.predict(X_test)\n",
    "y_pred_proba = smote_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# eval metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"AUROC: {auroc:.4f}\")\n",
    "print(\"\\nConfusion Matrix \\nTN FP \\nFN TP:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# feature importance\n",
    "feature_importance = smote_model.feature_importances_\n",
    "feature_names = X_train_smote.columns if hasattr(X_train_smote, 'columns') else [f'Feature_{i}' for i in range(X_train_smote.shape[1])]\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "plt.figure(figsize=(3,3))\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='0.5')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# precision recall curve\n",
    "plt.figure(figsize=(3,3))\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_pred_proba)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac38e1-a8f5-4efb-880c-3d341e22c822",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594355d-9ba0-4689-b320-b9d5db492d3f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# age/sex matched hyperparameter tuning\n",
    "\n",
    "def objective(trial):\n",
    "    # hyperparameters relevant to LGBM\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_bin': trial.suggest_int('max_bin', 128, 1024),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 128),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 1000, log=True),\n",
    "        'min_sum_hessian_in_leaf': trial.suggest_float('min_sum_hessian_in_leaf', 1e-4, 10, log=True),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 5000),\n",
    "        'n_jobs': -1,\n",
    "        'bagging_freq': 1,\n",
    "        'force_row_wise': True,\n",
    "        'bagging_seed': 2024,\n",
    "        'verbosity': -100,\n",
    "        'extra_trees': False}\n",
    "\n",
    "    # create model with above parameters\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    cv = KFold(n_splits = cv_folds, shuffle=True, random_state=1)\n",
    "    scores = cross_val_score(model, X_train_matched, y_train_matched, cv=cv, scoring='recall', n_jobs=1)\n",
    "    return scores.mean()\n",
    "\n",
    "# create and run the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=n_iterations, show_progress_bar=True)\n",
    "\n",
    "# get the best hyperparameters\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"\\nBest cross-validation score: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5399e-326b-481d-9790-793488b96765",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# age/sex matched model evaluation\n",
    "\n",
    "# set the optimal hyperparameters found above\n",
    "matched_params = {\n",
    "    'n_estimators': 312,\n",
    "    'max_bin': 733,\n",
    "    'learning_rate': 0.28642682596893726,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 123,\n",
    "    'bagging_fraction': 0.6412630511112203,\n",
    "    'feature_fraction': 0.7374540315935075,\n",
    "    'lambda_l2': 0.6432538398292775,\n",
    "    'min_sum_hessian_in_leaf': 1.1693341208203987,\n",
    "    'min_data_in_leaf': 324,\n",
    "\n",
    "    # normal params:\n",
    "    'n_jobs': -1,\n",
    "    'bagging_freq': 1,\n",
    "    'force_row_wise': True,\n",
    "    'bagging_seed': 2024,\n",
    "    'verbosity': -100,\n",
    "    'extra_trees': False,\n",
    "}\n",
    "\n",
    "# train the final model\n",
    "matched_model = lgb.LGBMClassifier(**matched_params)\n",
    "matched_model.fit(X_train_matched, y_train_matched)\n",
    "\n",
    "# make predictions with test set\n",
    "y_pred = matched_model.predict(X_test_matched)\n",
    "y_pred_proba = matched_model.predict_proba(X_test_matched)[:, 1]\n",
    "\n",
    "# eval metrics\n",
    "precision = precision_score(y_test_matched, y_pred)\n",
    "recall = recall_score(y_test_matched, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test_matched, y_pred)\n",
    "auroc = roc_auc_score(y_test_matched, y_pred_proba)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"AUROC: {auroc:.4f}\")\n",
    "print(\"\\nConfusion Matrix \\nTN FP \\nFN TP:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# feature importance\n",
    "feature_importance = matched_model.feature_importances_\n",
    "feature_names = X_train_matched.columns if hasattr(X_train_matched, 'columns') else [f'Feature_{i}' for i in range(X_train_matched.shape[1])]\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "plt.figure(figsize=(3,3))\n",
    "RocCurveDisplay.from_predictions(y_test_matched, y_pred_proba)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='0.5')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# precision recall curve\n",
    "plt.figure(figsize=(3,3))\n",
    "PrecisionRecallDisplay.from_predictions(y_test_matched, y_pred_proba)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
