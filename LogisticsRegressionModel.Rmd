---
title: "logistics regression"
author: "QianWang-qwan621"
date: "2025-09-04"
output: html_document
---

```{r}
library(tidyverse)
library(caret)
library(pROC)
library(ROSE)
library(vip)
```

```{r}
df <- read.csv("CVD_cleaned.csv", stringsAsFactors = FALSE)

dim(df)
str(df)
summary(df)
head(df)
```

# 1.preprocessing
```{r}
#
df$Heart_Disease <- factor(df$Heart_Disease,
                           levels = c("No", "Yes"))
cat("Level of the target variable：", levels(df$Heart_Disease), "\n")

#
categorical_cols <- c("General_Health", "Checkup", "Exercise", "Skin_Cancer", 
                      "Other_Cancer", "Depression", "Diabetes", "Arthritis", 
                      "Sex", "Age_Category", "Smoking_History")

df[categorical_cols] <- lapply(df[categorical_cols], factor)

# 
df_encoded <- dummyVars(Heart_Disease ~ ., data = df, fullRank = TRUE) %>%
  predict(newdata = df) %>%
  as.data.frame() %>%
  mutate(Heart_Disease = df$Heart_Disease)

# divide8:2
set.seed(123)
train_index <- createDataPartition(df_encoded$Heart_Disease, p = 0.8, list = FALSE)
train_data <- df_encoded[train_index, ]
test_data  <- df_encoded[-train_index, ]
```

# 2.Cross-validation and parameter tuning
```{r}
train_control <- trainControl(
  method = "cv",
  number = 5,
  search = "random",
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  savePredictions = "final",
  verboseIter = FALSE
)

tune_grid <- expand.grid(
  alpha = c(0, 1), # 0=ridge regression，1=Lasso
  lambda = exp(seq(-5, 2, length.out = 50))  #50 optional values (regularization intensity)）
)
```

# 3. model training
```{r}
# Preprocessing + Dummy encoding

#
train_data$Heart_Disease <- factor(train_data$Heart_Disease, levels = c("No", "Yes"))
test_data$Heart_Disease  <- factor(test_data$Heart_Disease, levels = c("No", "Yes"))

#
names(train_data) <- make.names(names(train_data))
names(test_data)  <- make.names(names(test_data))

# dummyVars
dummy_encoder <- dummyVars(Heart_Disease ~ ., data = train_data, fullRank = TRUE)

train_encoded <- predict(dummy_encoder, newdata = train_data) %>% as.data.frame()
train_encoded$Heart_Disease <- train_data$Heart_Disease

test_encoded <- predict(dummy_encoder, newdata = test_data) %>% as.data.frame()
test_encoded$Heart_Disease <- test_data$Heart_Disease
```

```{r}
# 3.1 Traditional logistic regression
set.seed(123)
model_logistic <- train(
  x = train_encoded %>% select(-Heart_Disease),
  y = train_encoded$Heart_Disease,
  method = "glmnet",
  tuneGrid = tune_grid,
  trControl = train_control,
  metric = "ROC",
  family = "binomial"
)
```

```{r}
# 3.2 Weighted logistic regression
n0 <- sum(train_encoded$Heart_Disease == "No")
n1 <- sum(train_encoded$Heart_Disease == "Yes")
weights <- ifelse(train_encoded$Heart_Disease == "Yes", n0/n1, 1)

set.seed(123)
model_weighted <- train(
  x = train_encoded %>% select(-Heart_Disease),
  y = train_encoded$Heart_Disease,
  method = "glmnet",
  tuneGrid = tune_grid,
  trControl = train_control,
  weights = weights,
  metric = "ROC",
  family = "binomial"
)
```

```{r}
# 3.3 SMOTE + Weighted logistic regression
#
formula_safe <- reformulate(setdiff(names(train_encoded), "Heart_Disease"), response = "Heart_Disease")
# ROSE
train_smote <- ROSE(formula_safe, data = train_encoded, seed = 123)$data
train_smote$Heart_Disease <- factor(train_smote$Heart_Disease, levels = c("No", "Yes"))
# SMOTE Sample weight
n0_smote <- sum(train_smote$Heart_Disease == "No")
n1_smote <- sum(train_smote$Heart_Disease == "Yes")
weights_smote <- ifelse(train_smote$Heart_Disease == "Yes", n0_smote / n1_smote, 1)

set.seed(123)
model_smote_weighted <- train(
  x = train_smote %>% select(-Heart_Disease),
  y = train_smote$Heart_Disease,
  method = "glmnet",
  tuneGrid = tune_grid,
  trControl = train_control,
  weights = weights_smote,
  metric = "ROC",
  family = "binomial"
)
```

```{r}
cat("The number of original training set：", nrow(train_encoded), "\n")
print(table(train_encoded$Heart_Disease))

cat("The number of smote training set：", nrow(train_smote), "\n")
print(table(train_smote$Heart_Disease)) 
```

# 4. Model prediction and evaluation
```{r}
actual <- test_data$Heart_Disease
#
feature_cols <- names(train_encoded %>% select(-Heart_Disease))
test_aligned <- test_encoded %>%
  mutate(across(.cols = setdiff(feature_cols, names(.)), .fns = ~0)) %>%
  select(all_of(feature_cols))

feature_cols_smote <- names(train_smote %>% select(-Heart_Disease))
test_aligned_smote <- test_encoded %>%
  mutate(across(.cols = setdiff(feature_cols_smote, names(.)), .fns = ~0)) %>%
  select(all_of(feature_cols_smote))

# prediction probability
pred_logistic       <- predict(model_logistic, test_aligned, type = "prob")[, "Yes"]
pred_weighted       <- predict(model_weighted, test_aligned, type = "prob")[, "Yes"]
pred_smote_weighted <- predict(model_smote_weighted, test_aligned_smote, type = "prob")[, "Yes"]

# Evaluation function
eval_metrics <- function(pred_prob, actual, threshold) {
  pred <- ifelse(pred_prob > threshold, "Yes", "No")
  cm <- confusionMatrix(factor(pred, levels = c("No", "Yes")), actual, positive = "Yes")
  auc <- roc(actual, pred_prob)$auc
  data.frame(
    Threshold = threshold, 
    Accuracy = cm$overall["Accuracy"], 
    Precision = cm$byClass["Precision"],
    Recall = cm$byClass["Recall"],
    F1_Score = cm$byClass["F1"],
    AUC = as.numeric(auc) 
  )
}

# Multi-threshold evaluation
thresholds <- c(0.3, 0.4, 0.5)
metrics_logistic <- lapply(thresholds, function(t) eval_metrics(pred_logistic, actual, t)) %>%
  bind_rows() %>% mutate(Model = "Logistic Regression")
metrics_weighted <- lapply(thresholds, function(t) eval_metrics(pred_weighted, actual, t)) %>%
  bind_rows() %>% mutate(Model = "Weighted Logistic Regression")
metrics_smote <- lapply(thresholds, function(t) eval_metrics(pred_smote_weighted, actual, t)) %>%
  bind_rows() %>% mutate(Model = "SMOTE + Weighted Logistic Regression")

all_metrics <- bind_rows(metrics_logistic, metrics_weighted, metrics_smote)
cat("=== Model Evaluation Metrics (Different Thresholds) ===\n")
print(all_metrics, digits = 3)
```

# 5. ROC-plot
```{r}
roc_logistic <- roc(actual, pred_logistic)
roc_weighted <- roc(actual, pred_weighted)
roc_smote    <- roc(actual, pred_smote_weighted)

ggroc(list(
  "Logistic Regression" = roc_logistic,
  "Weighted Logistic Regression" = roc_weighted,
  "SMOTE + Weighted Logistic Regression" = roc_smote
)) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  labs(title = "ROC Curve Comparison", x = "False Positive Rate (FPR)", y = "True Positive Rate (TPR)") +
  theme_minimal()
```

# 6. Comparison of recall rate and accuracy rate under different thresholds
```{r}
all_metrics_long <- all_metrics %>%
  pivot_longer(
    cols = c(Accuracy, Recall),  
    names_to = "Metric",         
    values_to = "Value"          
  )

ggplot(all_metrics_long, 
       aes(
         x = as.numeric(as.character(Threshold)), 
         y = Value, 
         color = Model,       
         linetype = Metric, 
         shape = Metric,  
         group = interaction(Model, Metric)
       )) +  
  geom_line(size = 1.2) + 
  geom_point(size = 4, stroke = 1) +
  scale_linetype_manual(
    values = c("Recall" = "solid", "Accuracy" = "dashed"),  
    name = "Metric"
  ) +  
  scale_shape_manual(
    values = c("Recall" = 16, "Accuracy" = 17),  
    name = "Metric"
  ) +  
  theme_minimal() +  
  theme(legend.position = "right") +  
  labs(
    title = "Recall vs. Accuracy Across Thresholds",  
    x = "Prediction Threshold",  
    y = "Value"  
  )
```

# confusion matrix
```{r}
#
generate_confusion_matrices <- function(pred_prob, actual, thresholds, model_name) {
  cm_list <- list()
  for (t in thresholds) {
    pred_class <- ifelse(pred_prob > t, "Yes", "No")
    cm <- confusionMatrix(
      reference = actual, 
      data = factor(pred_class, levels = c("No", "Yes")), 
      positive = "Yes"
    )
    cm_list[[paste0(model_name, "_Threshold_", t)]] <- cm
  }
  return(cm_list)
}

cm_logistic <- generate_confusion_matrices(pred_logistic, actual, thresholds, "Logistic Regression")
cm_weighted <- generate_confusion_matrices(pred_weighted, actual, thresholds, "Weighted Logistic Regression")
cm_smote    <- generate_confusion_matrices(pred_smote_weighted, actual, thresholds, "SMOTE + Weighted Logistic Regression")

cat("\n=== Logistic Regression - confusion matrix ===\n")
for (name in names(cm_logistic)) {
  cat("\n>>>", name, "<<<\n")
  print(cm_logistic[[name]])
}

cat("\n=== Weighted Logistic Regression - confusion matrix ===\n")
for (name in names(cm_weighted)) {
  cat("\n>>>", name, "<<<\n")
  print(cm_weighted[[name]])
}

cat("\n=== SMOTE + Weighted Logistic Regression - confusion matrix ===\n")
for (name in names(cm_smote)) {
  cat("\n>>>", name, "<<<\n")
  print(cm_smote[[name]])
}
```

# 7. Analysis of Variable Importance (Absolute value of coefficients)
```{r}
var_imp <- function(model) {
  coefs <- coef(model$finalModel, model$bestTune$lambda) %>%
    as.matrix() %>% as.data.frame() %>%
    rownames_to_column("Variable") %>%
    setNames(c("Variable", "Importance")) %>%
    filter(Variable != "(Intercept)") %>%
    mutate(Importance = abs(Importance)) %>%
    arrange(desc(Importance))
  return(coefs)
}
#
imp_logistic <- var_imp(model_logistic) %>% mutate(Model = "Logistic Regression")
imp_weighted <- var_imp(model_weighted) %>% mutate(Model = "Weighted Logistic Regression")
imp_smote    <- var_imp(model_smote_weighted) %>% mutate(Model = "SMOTE + Weighted Logistic Regression")
# Top10
cat("\n=== Logistic Regression Top10 Variable Importance ===\n")
print(head(imp_logistic, 10), digits = 3)
cat("\n=== Weighted Logistic Regression Top10 Variable Importance ===\n")
print(head(imp_weighted, 10), digits = 3)
cat("\n=== SMOTE + Weighted Logistic Regression Top10 Variable Importance ===\n")
print(head(imp_smote, 10), digits = 3)
# plot Top10
plot_top10 <- function(imp_df, title) {
  imp_df %>% head(10) %>%
    ggplot(aes(x = reorder(Variable, Importance), y = Importance)) +
    geom_bar(stat = "identity", fill = "#1E90FF") +
    coord_flip() +
    labs(title = title, x = "Variable", y = "Importance (|Coefficient|)") +
    theme_minimal()
}

plot_top10(imp_logistic, "Logistic Regression Top10 Variable Importance")
plot_top10(imp_weighted, "Weighted Logistic Regression Top10 Variable Importance")
plot_top10(imp_smote, "SMOTE + Weighted Logistic Regression Top10 Variable Importance")
```

# 7-2. Analysis of Variable Importance (Combined)
```{r}
library(tidyverse)
library(stringr)  

var_imp <- function(model) {
  coefs <- coef(model$finalModel, model$bestTune$lambda) %>%
    as.matrix() %>% as.data.frame() %>%
    rownames_to_column("Variable") %>%
    setNames(c("Variable", "Importance")) %>%
    filter(Variable != "(Intercept)") %>% 
    mutate(
      Importance = abs(Importance), 
      Original_Variable = str_extract(Variable, "^[^\\.]+")
    ) %>%
    group_by(Original_Variable) %>% 
    summarise(Importance = sum(Importance), 
              .groups = "drop") %>%
    arrange(desc(Importance))  
  return(coefs)
}

imp_logistic <- var_imp(model_logistic) %>% mutate(Model = "Logistic Regression")
imp_weighted <- var_imp(model_weighted) %>% mutate(Model = "Weighted Logistic Regression")
imp_smote    <- var_imp(model_smote_weighted) %>% mutate(Model = "SMOTE + Weighted Logistic Regression")

# Top10-combine
cat("\n=== Logistic Regression Top10 Variable Importance（合并后） ===\n")
print(head(imp_logistic, 10), digits = 3)
cat("\n=== Weighted Logistic Regression Top10 Variable Importance（合并后） ===\n")
print(head(imp_weighted, 10), digits = 3)
cat("\n=== SMOTE + Weighted Logistic Regression Top10 Variable Importance（合并后） ===\n")
print(head(imp_smote, 10), digits = 3)

# plot Top10-combine
plot_top10 <- function(imp_df, title) {
  imp_df %>% head(10) %>%
    ggplot(aes(x = reorder(Original_Variable, Importance), y = Importance)) +
    geom_bar(stat = "identity", fill = "#1E90FF") +
    coord_flip() +
    labs(title = title, 
         x = "Variable", 
         y = "Importance (Sum of |Coefficients|)") +  # 说明是系数和
    theme_minimal()
}

plot_top10(imp_logistic, "Logistic Regression Top10 Variable Importance（合并后）")
plot_top10(imp_weighted, "Weighted Logistic Regression Top10 Variable Importance（合并后）")
plot_top10(imp_smote, "SMOTE + Weighted Logistic Regression Top10 Variable Importance（合并后）")
```

# 8. Extract the optimal parameters + construct the logit equation
```{r}
print_model_details <- function(model, model_name) {
  best_params <- model$bestTune
  cat("\n=== ", model_name, " Best Hyperparameters ===\n")
  print(best_params)
  
  coefs <- coef(model$finalModel, s = best_params$lambda)
  coefs <- as.matrix(coefs)
  coefs_df <- data.frame(
    Variable = rownames(coefs),
    Coefficient = coefs[,1], 
    stringsAsFactors = FALSE
  )
  
  intercept <- dplyr::filter(coefs_df, Variable == "(Intercept)")$Coefficient
  intercept <- round(intercept, 3)
  
  feature_coefs <- dplyr::filter(coefs_df, Variable != "(Intercept)")
  feature_coefs$Coefficient <- round(feature_coefs$Coefficient, 3)
  
  formula_str <- paste0("logit(P(Heart_Disease='Yes')) = ", intercept, " + ")
  for(i in 1:nrow(feature_coefs)) {
    formula_str <- paste0(formula_str, feature_coefs$Coefficient[i], " * ", feature_coefs$Variable[i], " + ")
  }
  formula_str <- substr(formula_str, 1, nchar(formula_str)-3)  # 去掉最后 " + "
  
  cat("\n=== ", model_name, " Logit Equation ===\n")
  cat(formula_str, "\n\n")
}

print_model_details(model_logistic, "Logistic Regression")
print_model_details(model_weighted, "Weighted Logistic Regression")
print_model_details(model_smote_weighted, "SMOTE + Weighted Logistic Regression")
```
