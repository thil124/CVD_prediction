
import optuna
from sklearn.model_selection import StratifiedKFold, cross_val_score

N_TRIALS = 10      
CV_FOLDS = 5      
THRESHOLD = 0      
MAX_ITER = 5000
TOL = 1e-3
RANDOM_STATE = 1

def objective(trial):
    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])
    C = trial.suggest_float('C', 1e-6, 10.0, log=True)

    params = dict(kernel=kernel, C=C)
    if kernel in ('rbf', 'poly'):
        params['gamma'] = trial.suggest_float('gamma', 1e-6, 10.0, log=True)
    if kernel == 'poly':
        params['degree'] = trial.suggest_int('degree', 2, 3)

    model = Pipeline([
        ("scaler", StandardScaler()),
        ("svc", SVC(
            class_weight="balanced",
            probability=False,
            max_iter=MAX_ITER,
            tol=TOL,
            random_state=RANDOM_STATE,
            **params
        ))
    ])

    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)
  
    return cross_val_score(model, X_train, y_train,
                           scoring="roc_auc", cv=cv, n_jobs=-1).mean()

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=N_TRIALS)


top_trials = sorted(study.trials, key=lambda t: t.value if t.value is not None else -1, reverse=True)[:3]


from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_auc_score, RocCurveDisplay
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

print("=" * 80)
print(f"Best CV ROC AUC: {study.best_value:.5f}")
print("Best Params:", study.best_params)
print("=" * 80)

print("=" * 80)
print(f"Comparison Results for Top-{len(top_trials)} Optuna Configs (Threshold = {THRESHOLD}):")
print("=" * 80)

results = []
all_y_scores = []
labels = []

for rank, tr in enumerate(top_trials, start=1):
    p = tr.params.copy()
    kernel = p.get('kernel')
    C = float(p.get('C'))
    gamma = p.get('gamma', None)
    degree = p.get('degree', None)

    svc_kwargs = dict(kernel=kernel, C=C)
    if gamma is not None:
        svc_kwargs['gamma'] = float(gamma)   # 仅 rbf/poly
    if degree is not None:
        svc_kwargs['degree'] = int(degree)   # 仅 poly

    final_model = Pipeline([
        ("scaler", StandardScaler()),
        ("svc", SVC(
            class_weight="balanced",
            probability=False,
            max_iter=MAX_ITER,
            tol=TOL,
            random_state=RANDOM_STATE,
            **svc_kwargs
        ))
    ])

    final_model.fit(X_train, y_train)
    y_score = final_model.decision_function(X_test)
    all_y_scores.append(y_score)

    y_pred = (y_score > THRESHOLD).astype(int)

    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    specificity = tn / (tn + fp) if (tn+fp)>0 else float("nan")
    npv = tn / (tn + fn) if (tn+fn)>0 else float("nan")
    auroc = roc_auc_score(y_test, y_score)

    label = f"#{rank}: kernel={kernel}, C={C:.3g}" + \
            (f", γ={gamma:.3g}" if gamma is not None else "") + \
            (f", deg={degree}" if degree is not None else "")
    labels.append(label)

    results.append({
        'label': label,
        'kernel': kernel,
        'C': C,
        'gamma': gamma,
        'degree': degree,
        'auroc': auroc,
        'precision': precision,
        'recall': recall,
        'specificity': specificity,
        'npv': npv,
        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp
    })

    print(f"\n{label}")
    print(f"AUROC: {auroc:.4f}")
    print(f"Precision (PPV): {precision:.4f}")
    print(f"Recall (Sensitivity): {recall:.4f}")
    print(f"Specificity: {specificity:.4f}")
    print(f"NPV: {npv:.4f}")
    print(f"Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}")

print("=" * 80)

# === Confusion Matrices ===
fig, axes = plt.subplots(1, len(results), figsize=(6*len(results), 5))
if len(results) == 1:
    axes = [axes]

for idx, result in enumerate(results):
    cm = np.array([[result['tn'], result['fp']],
                   [result['fn'], result['tp']]])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No CVD','CVD'],
                yticklabels=['No CVD','CVD'],
                ax=axes[idx])
    axes[idx].set_title(f"{result['label']}\nPPV={result['precision']:.3f}, Sens={result['recall']:.3f}\nAUROC={result['auroc']:.3f}")
    axes[idx].set_xlabel('Predicted')
    axes[idx].set_ylabel('True')

plt.tight_layout()
plt.show()

# === ROC Curves ===
plt.figure(figsize=(8, 6))
for idx, (label, y_score) in enumerate(zip(labels, all_y_scores)):
    auroc = results[idx]['auroc']
    RocCurveDisplay.from_predictions(
        y_test, y_score,
        name=f'{label} (AUROC={auroc:.3f})',
        plot_chance_level=(idx==0)
    )
plt.plot([0,1],[0,1],'k--', label='Chance level')
plt.title(f"ROC Curves Comparison (SVM, balanced, Threshold={THRESHOLD})")
plt.legend()
plt.show()
